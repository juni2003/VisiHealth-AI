{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a43f105",
   "metadata": {},
   "source": [
    "# üì¶ PART 1: SETUP\n",
    "\n",
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify connection\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "print(\"\\nContents of your Drive:\")\n",
    "!ls -lh /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131433d",
   "metadata": {},
   "source": [
    "## Step 2: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a97296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(\"‚úÖ GPU is ready!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Go to: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb59e8fb",
   "metadata": {},
   "source": [
    "## Step 3: Copy Project from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de008b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set paths to your uploaded folders on Google Drive\n",
    "# ‚ö†Ô∏è CHANGE THESE to match your Google Drive structure\n",
    "PROJECT_FOLDER = '/content/drive/MyDrive/VISIHEALTH CODE'\n",
    "SLAKE_FOLDER = '/content/drive/MyDrive/Slake1.0'\n",
    "\n",
    "# Check if folders exist\n",
    "if not os.path.exists(PROJECT_FOLDER):\n",
    "    print(f\"‚ùå ERROR: Project folder not found at: {PROJECT_FOLDER}\")\n",
    "    print(\"\\nYour Drive contents:\")\n",
    "    !ls -lh /content/drive/MyDrive/\n",
    "    print(\"\\nüëÜ Update PROJECT_FOLDER path to match your folder name above\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found project folder: {PROJECT_FOLDER}\")\n",
    "\n",
    "if not os.path.exists(SLAKE_FOLDER):\n",
    "    print(f\"‚ùå ERROR: SLAKE folder not found at: {SLAKE_FOLDER}\")\n",
    "    print(\"\\nYour Drive contents:\")\n",
    "    !ls -lh /content/drive/MyDrive/\n",
    "    print(\"\\nüëÜ Update SLAKE_FOLDER path to match your folder name above\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found SLAKE folder: {SLAKE_FOLDER}\")\n",
    "\n",
    "# Copy entire project to Colab workspace for faster access\n",
    "print(\"\\nüì¶ Copying project to Colab workspace...\")\n",
    "!cp -r \"{PROJECT_FOLDER}\" /content/VisiHealth\n",
    "\n",
    "# Change to project directory\n",
    "%cd /content/VisiHealth\n",
    "\n",
    "print(\"\\n‚úÖ Project copied!\")\n",
    "print(\"\\nProject structure:\")\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607040fc",
   "metadata": {},
   "source": [
    "## Step 4: Copy SLAKE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c614b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "!mkdir -p data/SLAKE\n",
    "\n",
    "# Check if dataset already exists to avoid re-copying\n",
    "import os\n",
    "\n",
    "if os.path.exists('data/SLAKE/train.json') and os.path.exists('data/SLAKE/imgs'):\n",
    "    num_images = len(os.listdir('data/SLAKE/imgs')) if os.path.exists('data/SLAKE/imgs') else 0\n",
    "    print(f\"‚úÖ Dataset already loaded! Found {num_images} images\")\n",
    "    print(\"Skipping copy to save time...\")\n",
    "else:\n",
    "    # Copy SLAKE dataset from Google Drive (this may take 5-10 minutes)\n",
    "    print(\"üìä Copying SLAKE dataset...\")\n",
    "    print(\"This will take a few minutes for 642 images...\")\n",
    "    !cp -r {SLAKE_FOLDER}/* data/SLAKE/\n",
    "    print(\"‚úÖ Dataset copied!\")\n",
    "\n",
    "print(\"\\nDataset structure:\")\n",
    "!ls -lh data/SLAKE/\n",
    "print(\"\\nNumber of image folders:\")\n",
    "!ls data/SLAKE/imgs/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae906c3",
   "metadata": {},
   "source": [
    "## Step 5: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Install Dependencies (only runs if packages not already installed)\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "# Check if packages are already installed\n",
    "try:\n",
    "    import cv2\n",
    "    import transformers\n",
    "    packages_installed = True\n",
    "    print(\"‚úÖ Packages already installed. Skipping installation...\")\n",
    "except ImportError:\n",
    "    packages_installed = False\n",
    "\n",
    "if not packages_installed:\n",
    "    print(\"üì¶ Installing dependencies...\")\n",
    "    \n",
    "    # Uninstall conflicting packages first\n",
    "    !pip uninstall -y opencv-python opencv-contrib-python opencv-python-headless -q\n",
    "    \n",
    "    # Install packages in specific order\n",
    "    !pip install -q \"numpy<2.0\"\n",
    "    !pip install -q opencv-python-headless==4.10.0.84\n",
    "    !pip install -q transformers==4.35.0\n",
    "    !pip install -q \"huggingface-hub>=0.20.0,<1.0\"\n",
    "    !pip install -q albumentations==1.4.0\n",
    "    !pip install -q tensorboard scikit-learn tqdm pyyaml pillow matplotlib\n",
    "    \n",
    "    print(\"\\n‚úÖ Installation complete!\")\n",
    "    print(\"\\n‚ö†Ô∏è Please restart runtime manually and run all cells again.\")\n",
    "else:\n",
    "    print(\"‚úÖ Ready to continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3418b25",
   "metadata": {},
   "source": [
    "## Step 6: Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Verify Setup\n",
    "\n",
    "print(\"üß™ Verifying setup...\\n\")\n",
    "\n",
    "# First, verify Python packages\n",
    "print(\"=\" * 60)\n",
    "print(\"Checking installed packages...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "    print(f\"‚úÖ Transformers: {transformers.__version__}\")\n",
    "    print(f\"‚úÖ OpenCV: {cv2.__version__}\")\n",
    "    print(f\"‚úÖ NumPy: {np.__version__}\")\n",
    "    \n",
    "    # Verify NumPy is version 1.x\n",
    "    if np.__version__.startswith('2'):\n",
    "        print(\"\\n‚ö†Ô∏è WARNING: NumPy 2.x detected. May cause issues.\")\n",
    "        print(\"   Consider restarting runtime and running Step 5 again.\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All packages verified!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå Package import failed: {e}\")\n",
    "    print(\"\\nüî¥ CRITICAL ERROR:\")\n",
    "    print(\"   Packages not installed correctly or runtime not restarted.\")\n",
    "    print(\"\\nüìã To fix:\")\n",
    "    print(\"   1. Go back to Step 5\")\n",
    "    print(\"   2. Run Step 5 again\")\n",
    "    print(\"   3. Restart runtime: Runtime ‚Üí Restart runtime\")\n",
    "    print(\"   4. Re-run from Step 1\")\n",
    "    raise\n",
    "\n",
    "# Now check project files\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Checking project files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import os\n",
    "core_files = [\n",
    "    'models/cnn_model.py',\n",
    "    'models/bert_model.py',\n",
    "    'models/fusion_model.py',\n",
    "    'scripts/train.py',\n",
    "    'scripts/demo.py',\n",
    "    'data/dataset.py',\n",
    "    'utils/knowledge_graph.py',\n",
    "    'config.yaml',\n",
    "    'requirements.txt'\n",
    "]\n",
    "\n",
    "all_good = True\n",
    "for file in core_files:\n",
    "    exists = \"‚úÖ\" if os.path.exists(file) else \"‚ùå\"\n",
    "    print(f\"  {exists} {file}\")\n",
    "    if not os.path.exists(file):\n",
    "        all_good = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Checking dataset...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists('data/SLAKE/train.json'):\n",
    "    print(\"  ‚úÖ data/SLAKE/train.json\")\n",
    "    print(\"  ‚úÖ data/SLAKE/test.json\")\n",
    "    print(\"  ‚úÖ data/SLAKE/imgs/\")\n",
    "else:\n",
    "    print(\"  ‚ùå Dataset not found!\")\n",
    "    all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ SETUP VERIFICATION COMPLETE! READY TO TRAIN.\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚ö†Ô∏è SOME FILES ARE MISSING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Check the paths in Step 3 and Step 4, then try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b823d3d",
   "metadata": {},
   "source": [
    "## Step 7: Create Checkpoint Directories in Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories in Google Drive to save checkpoints permanently\n",
    "!mkdir -p /content/drive/MyDrive/VisiHealth_Checkpoints\n",
    "!mkdir -p /content/drive/MyDrive/VisiHealth_Logs\n",
    "\n",
    "# Create symlinks to save directly to Drive\n",
    "!ln -sf /content/drive/MyDrive/VisiHealth_Checkpoints /content/VisiHealth/checkpoints\n",
    "!ln -sf /content/drive/MyDrive/VisiHealth_Logs /content/VisiHealth/logs\n",
    "\n",
    "print(\"‚úÖ Checkpoints will be saved to Google Drive\")\n",
    "print(\"   Location: MyDrive/VisiHealth_Checkpoints/\")\n",
    "print(\"‚úÖ Logs will be saved to Google Drive\")\n",
    "print(\"   Location: MyDrive/VisiHealth_Logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d3114",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üèãÔ∏è PART 2: TRAINING\n",
    "\n",
    "## Step 8: Load and Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a24572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from data import get_dataloader\n",
    "\n",
    "# Load config\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"üìã Loading SLAKE dataset...\\n\")\n",
    "\n",
    "# Load train dataset\n",
    "train_loader, train_dataset = get_dataloader(\n",
    "    data_dir='data/SLAKE',\n",
    "    split='train',\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    num_workers=2,\n",
    "    tokenizer_name=config['model']['bert']['model_name']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"‚úÖ Number of classes: {train_dataset.num_classes}\")\n",
    "print(f\"‚úÖ Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"‚úÖ Total batches: {len(train_loader)}\")\n",
    "\n",
    "# Visualize a few samples\n",
    "print(\"\\nüì∏ Visualizing sample data...\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i >= len(sample_batch['image']):\n",
    "        break\n",
    "    \n",
    "    img = sample_batch['image'][i].cpu().numpy().transpose(1, 2, 0)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    question = sample_batch['question_text'][i][:50] + \"...\"\n",
    "    answer = sample_batch['answer_text'][i]\n",
    "    ax.set_title(f\"Q: {question}\\nA: {answer}\", fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Dataset loaded and verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11fc5fa",
   "metadata": {},
   "source": [
    "## Step 9: Start Training\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:**\n",
    "- Training will take 2-4 hours on T4 GPU\n",
    "- Checkpoints are automatically saved to Google Drive\n",
    "- You can monitor progress in real-time below\n",
    "- If session disconnects, your checkpoint is safe in Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training (with auto-resume from checkpoint if available)\n",
    "print(\"üèãÔ∏è Starting training...\\n\")\n",
    "print(\"This will:\")\n",
    "print(\"  - Auto-resume from latest checkpoint if available\")\n",
    "print(\"  - Train for up to 50 epochs (with early stopping)\")\n",
    "print(\"  - Save checkpoints to Google Drive every 5 epochs\")\n",
    "print(\"  - Log metrics for TensorBoard\")\n",
    "print(\"  - Show progress bars for each epoch\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Always use --resume flag to automatically resume if checkpoints exist\n",
    "!python scripts/train.py --resume\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57055cb",
   "metadata": {},
   "source": [
    "## Step 10: Monitor Training with TensorBoard (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd9ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard to visualize training metrics\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881737d",
   "metadata": {},
   "source": [
    "## Step 11: Check Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "\n",
    "# List checkpoints\n",
    "checkpoints = glob.glob('/content/drive/MyDrive/VisiHealth_Checkpoints/*.pth')\n",
    "print(f\"üìä Found {len(checkpoints)} checkpoint(s):\")\n",
    "for ckpt in sorted(checkpoints):\n",
    "    size_mb = os.path.getsize(ckpt) / (1024*1024)\n",
    "    print(f\"  - {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "\n",
    "# Check best checkpoint\n",
    "best_ckpt = '/content/drive/MyDrive/VisiHealth_Checkpoints/best_checkpoint.pth'\n",
    "if os.path.exists(best_ckpt):\n",
    "    print(\"\\n‚úÖ Best checkpoint found!\")\n",
    "    \n",
    "    # Load checkpoint info\n",
    "    checkpoint = torch.load(best_ckpt, map_location='cpu')\n",
    "    print(f\"\\nBest model metrics:\")\n",
    "    print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"  Best Val Accuracy: {checkpoint.get('best_val_acc', 0):.2f}%\")\n",
    "    print(f\"  Best Val Loss: {checkpoint.get('best_val_loss', 0):.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No best checkpoint found yet.\")\n",
    "    print(\"Training may still be in progress or hasn't completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b73c40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîÆ PART 3: INFERENCE\n",
    "\n",
    "## Step 12: Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from models import get_cnn_model, get_bert_model, build_visihealth_model\n",
    "from data import get_dataloader\n",
    "\n",
    "# Load config\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load test dataset\n",
    "print(\"üìä Loading test dataset...\")\n",
    "test_loader, test_dataset = get_dataloader(\n",
    "    data_dir='data/SLAKE',\n",
    "    split='test',\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    "    tokenizer_name=config['model']['bert']['model_name']\n",
    ")\n",
    "print(f\"‚úÖ Test dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "# Build model\n",
    "print(\"\\nüß† Building model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "cnn = get_cnn_model(config)\n",
    "bert = get_bert_model(config)\n",
    "model = build_visihealth_model(config, cnn, bert)\n",
    "\n",
    "# Load checkpoint\n",
    "CHECKPOINT_PATH = '/content/drive/MyDrive/VisiHealth_Checkpoints/best_checkpoint.pth'\n",
    "print(f\"üì¶ Loading checkpoint from: {CHECKPOINT_PATH}\")\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Trained for {checkpoint.get('epoch', 'N/A')} epochs\")\n",
    "print(f\"   Best validation accuracy: {checkpoint.get('best_val_acc', 0):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1d6cf6",
   "metadata": {},
   "source": [
    "## Step 13: Run Inference on Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1991cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Get random samples\n",
    "num_samples = 6\n",
    "indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n",
    "\n",
    "correct_predictions = 0\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flat\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    sample = test_dataset[idx]\n",
    "    \n",
    "    # Prepare input\n",
    "    image = sample['image'].unsqueeze(0).to(device)\n",
    "    input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "    attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image, input_ids, attention_mask, return_attention=True)\n",
    "        answer_logits = outputs['answer_logits']\n",
    "        roi_scores = outputs['roi_scores']\n",
    "    \n",
    "    # Get predictions\n",
    "    answer_probs = F.softmax(answer_logits, dim=1)\n",
    "    pred_idx = answer_logits.argmax(dim=1).item()\n",
    "    confidence = answer_probs[0, pred_idx].item()\n",
    "    \n",
    "    pred_answer = test_dataset.get_answer_text(pred_idx)\n",
    "    true_answer = sample['answer_text']\n",
    "    \n",
    "    is_correct = pred_answer.lower() == true_answer.lower()\n",
    "    if is_correct:\n",
    "        correct_predictions += 1\n",
    "    \n",
    "    # Get top ROI\n",
    "    top_roi_idx = roi_scores.argmax(dim=1).item()\n",
    "    roi_confidence = F.softmax(roi_scores, dim=1)[0, top_roi_idx].item()\n",
    "    \n",
    "    # Visualize\n",
    "    img_display = sample['image'].cpu().numpy().transpose(1, 2, 0)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img_display = std * img_display + mean\n",
    "    img_display = np.clip(img_display, 0, 1)\n",
    "    \n",
    "    axes[i].imshow(img_display)\n",
    "    \n",
    "    status = \"‚úÖ CORRECT\" if is_correct else \"‚ùå WRONG\"\n",
    "    title = (\n",
    "        f\"Q: {sample['question_text'][:40]}...\\n\"\n",
    "        f\"Pred: {pred_answer} ({confidence:.1%}) | True: {true_answer}\\n\"\n",
    "        f\"{status} | ROI: {top_roi_idx} ({roi_confidence:.1%})\"\n",
    "    )\n",
    "    axes[i].set_title(title, fontsize=8)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Accuracy on these samples: {correct_predictions}/{num_samples} ({100*correct_predictions/num_samples:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37bfd05",
   "metadata": {},
   "source": [
    "## Step 14: Generate Rationales with Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.knowledge_graph import load_knowledge_graph, RationaleGenerator\n",
    "\n",
    "# Load KG\n",
    "kg_file = 'data/SLAKE/kg.txt'\n",
    "if os.path.exists(kg_file):\n",
    "    kg = load_knowledge_graph(kg_file)\n",
    "    rationale_gen = RationaleGenerator(kg)\n",
    "    print(f\"‚úÖ Knowledge graph loaded: {len(kg.triplets)} triplets\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è KG file not found. Creating sample...\")\n",
    "    os.makedirs(os.path.dirname(kg_file), exist_ok=True)\n",
    "    with open(kg_file, 'w') as f:\n",
    "        f.write(\"liver\\tis_located_in\\tabdomen\\n\")\n",
    "        f.write(\"lung\\tis_located_in\\tchest\\n\")\n",
    "        f.write(\"heart\\tis_located_in\\tchest\\n\")\n",
    "    kg = load_knowledge_graph(kg_file)\n",
    "    rationale_gen = RationaleGenerator(kg)\n",
    "\n",
    "# Generate rationales for first 3 samples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING RATIONALES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx in indices[:3]:\n",
    "    sample = test_dataset[idx]\n",
    "    \n",
    "    # Prepare input\n",
    "    image = sample['image'].unsqueeze(0).to(device)\n",
    "    input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "    attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image, input_ids, attention_mask)\n",
    "        answer_logits = outputs['answer_logits']\n",
    "        roi_scores = outputs['roi_scores']\n",
    "    \n",
    "    # Get predictions\n",
    "    answer_probs = F.softmax(answer_logits, dim=1)\n",
    "    pred_idx = answer_logits.argmax(dim=1).item()\n",
    "    confidence = answer_probs[0, pred_idx].item()\n",
    "    pred_answer = test_dataset.get_answer_text(pred_idx)\n",
    "    \n",
    "    # Get top ROIs\n",
    "    top_k_rois = torch.topk(roi_scores[0], k=3)\n",
    "    \n",
    "    # Generate rationale\n",
    "    rationale = rationale_gen.generate_rationale(\n",
    "        predicted_answer=pred_answer,\n",
    "        confidence=confidence,\n",
    "        top_roi_indices=top_k_rois.indices.tolist(),\n",
    "        roi_scores=top_k_rois.values.tolist(),\n",
    "        question=sample['question_text']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Image: {sample['img_name']}\")\n",
    "    print(f\"Question: {sample['question_text']}\")\n",
    "    print(f\"True Answer: {sample['answer_text']}\")\n",
    "    print(f\"Predicted Answer: {pred_answer} (confidence: {confidence:.2%})\")\n",
    "    print(f\"\\nüìù Rationale:\\n{rationale}\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c1260",
   "metadata": {},
   "source": [
    "## Step 15: Calculate Full Test Set Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20872c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Evaluate on entire test set\n",
    "print(\"üìä Evaluating on full test set...\\n\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Reload with batch size for faster evaluation\n",
    "test_loader_batch, _ = get_dataloader(\n",
    "    data_dir='data/SLAKE',\n",
    "    split='test',\n",
    "    batch_size=16,\n",
    "    num_workers=2,\n",
    "    tokenizer_name=config['model']['bert']['model_name']\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_batch, desc=\"Evaluating\"):\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        answers = batch['answer'].to(device)\n",
    "        \n",
    "        outputs = model(images, input_ids, attention_mask)\n",
    "        predictions = outputs['answer_logits'].argmax(dim=1)\n",
    "        \n",
    "        correct += (predictions == answers).sum().item()\n",
    "        total += answers.size(0)\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä TEST SET RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total Samples: {total}\")\n",
    "print(f\"Correct Predictions: {correct}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'test_accuracy': accuracy,\n",
    "    'correct': correct,\n",
    "    'total': total,\n",
    "    'checkpoint': CHECKPOINT_PATH\n",
    "}\n",
    "\n",
    "import json\n",
    "results_file = '/content/drive/MyDrive/VisiHealth_Results.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184fbf29",
   "metadata": {},
   "source": [
    "## Step 16: Export Model Info for Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70268e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model info for frontend integration\n",
    "export_info = {\n",
    "    'model_type': 'VisiHealth AI',\n",
    "    'checkpoint_path': CHECKPOINT_PATH,\n",
    "    'test_accuracy': accuracy,\n",
    "    'num_classes': test_dataset.num_classes,\n",
    "    'answer_vocab': {v: k for k, v in test_dataset.answer_vocab.items()},\n",
    "    'image_size': 224,\n",
    "    'bert_model': config['model']['bert']['model_name'],\n",
    "    'usage': {\n",
    "        'input': 'Medical image (224x224) + question text',\n",
    "        'output': 'Answer + confidence + ROI scores + rationale'\n",
    "    },\n",
    "    'training_info': {\n",
    "        'dataset': 'SLAKE 1.0',\n",
    "        'total_samples': len(train_dataset),\n",
    "        'epochs': checkpoint.get('epoch', 'N/A'),\n",
    "        'best_val_acc': checkpoint.get('best_val_acc', 0)\n",
    "    }\n",
    "}\n",
    "\n",
    "info_file = '/content/drive/MyDrive/VisiHealth_Model_Info.json'\n",
    "with open(info_file, 'w') as f:\n",
    "    json.dump(export_info, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model info exported to: {info_file}\")\n",
    "print(\"\\nThis file contains:\")\n",
    "print(\"  - Model checkpoint path\")\n",
    "print(\"  - Test accuracy\")\n",
    "print(\"  - Answer vocabulary (for mapping predictions)\")\n",
    "print(\"  - Input/output specifications\")\n",
    "print(\"  - Training information\")\n",
    "print(\"\\nüåê Ready for frontend integration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08269d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ ALL DONE!\n",
    "\n",
    "## üéâ Complete Workflow Finished!\n",
    "\n",
    "### What Was Accomplished:\n",
    "1. ‚úÖ **Setup Complete** - Project and dataset copied, dependencies installed\n",
    "2. ‚úÖ **Training Complete** - Model trained on SLAKE dataset with GPU\n",
    "3. ‚úÖ **Inference Complete** - Model tested, accuracy calculated, results saved\n",
    "\n",
    "### Files Saved to Google Drive:\n",
    "- üìÅ **VisiHealth_Checkpoints/** - Model checkpoints\n",
    "  - `best_checkpoint.pth` - Best model (~500MB)\n",
    "  - `checkpoint_epoch_XX.pth` - Regular checkpoints\n",
    "- üìÅ **VisiHealth_Logs/** - TensorBoard training logs\n",
    "- üìÑ **VisiHealth_Results.json** - Test accuracy and metrics\n",
    "- üìÑ **VisiHealth_Model_Info.json** - Model specifications for frontend\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download checkpoint** from Google Drive to your laptop:\n",
    "   - Go to: https://drive.google.com\n",
    "   - Navigate to: `VisiHealth_Checkpoints/`\n",
    "   - Download: `best_checkpoint.pth`\n",
    "\n",
    "2. **Use locally** on your laptop:\n",
    "   ```bash\n",
    "   python scripts/demo.py --checkpoint checkpoints/best_checkpoint.pth\n",
    "   ```\n",
    "\n",
    "3. **Build frontend** (Flask/FastAPI backend + React/Vue frontend)\n",
    "\n",
    "### Performance Summary:\n",
    "- **Test Accuracy:** Shown above\n",
    "- **Training Time:** ~2-4 hours\n",
    "- **Model Size:** ~500 MB\n",
    "- **Inference Speed:** ~200-300ms per image (GPU)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Congratulations! Your Medical VQA System is Ready!\n",
    "\n",
    "**Questions? Check the documentation in your project folder.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
