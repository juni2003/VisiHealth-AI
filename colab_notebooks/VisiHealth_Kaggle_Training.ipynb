{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6d780b",
   "metadata": {},
   "source": [
    "## üìã PART 1: SETUP & ENVIRONMENT\n",
    "\n",
    "### Step 1: Check GPU and System Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"üñ•Ô∏è KAGGLE ENVIRONMENT INFO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "    print(\"‚úÖ GPU is ready!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected!\")\n",
    "    print(\"   Go to: Settings ‚Üí Accelerator ‚Üí GPU T4 x2\")\n",
    "\n",
    "print(\"\\nüìÅ Working Directory:\", os.getcwd())\n",
    "print(\"üìÅ Available Space:\")\n",
    "os.system('df -h /kaggle/working')\n",
    "\n",
    "print(\"\\n‚úÖ System check complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8adb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"üñ•Ô∏è KAGGLE ENVIRONMENT INFO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "    print(\"‚úÖ GPU is ready!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected!\")\n",
    "    print(\"   Go to: Settings ‚Üí Accelerator ‚Üí GPU T4 x2\")\n",
    "\n",
    "print(\"\\nüìÅ Working Directory:\", os.getcwd())\n",
    "print(\"üìÅ Available Space:\")\n",
    "os.system('df -h /kaggle/working')\n",
    "\n",
    "print(\"\\n‚úÖ System check complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b94078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "print(\"This will take 2-3 minutes...\\n\")\n",
    "\n",
    "# Step 1: Aggressively uninstall ALL OpenCV variants\n",
    "print(\"üßπ Cleaning up existing OpenCV installations...\")\n",
    "!pip uninstall -y opencv-python opencv-contrib-python opencv-python-headless opencv-contrib-python-headless -qqq 2>/dev/null || true\n",
    "\n",
    "# Step 2: Install NumPy first (critical for compatibility)\n",
    "print(\"üìå Installing NumPy 1.26.4...\")\n",
    "!pip install -q \"numpy==1.26.4\"\n",
    "\n",
    "# Step 3: Install compatible OpenCV version\n",
    "print(\"üìå Installing OpenCV 4.8.0.76...\")\n",
    "!pip install -q opencv-python-headless==4.8.0.76\n",
    "\n",
    "# Step 4: Install albumentations with pinned version\n",
    "print(\"üìå Installing albumentations 1.3.1...\")\n",
    "!pip install -q albumentations==1.3.1\n",
    "\n",
    "# Step 5: Install other dependencies\n",
    "print(\"üìå Installing other packages...\")\n",
    "!pip install -q \\\n",
    "    transformers==4.35.0 \\\n",
    "    \"huggingface-hub>=0.20.0,<1.0\" \\\n",
    "    tensorboard \\\n",
    "    scikit-learn \\\n",
    "    tqdm \\\n",
    "    pyyaml \\\n",
    "    pillow \\\n",
    "    matplotlib\n",
    "\n",
    "# Step 6: Ensure NumPy stays at 1.26.4\n",
    "!pip install -q --force-reinstall \"numpy==1.26.4\"\n",
    "\n",
    "print(\"\\n‚úÖ Installation complete!\")\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT: RESTART THE KERNEL NOW!\")\n",
    "print(\"   Click: Runtime ‚Üí Restart Runtime (or Kernel ‚Üí Restart)\")\n",
    "print(\"   Then re-run this cell to verify imports.\\n\")\n",
    "\n",
    "# Verify imports\n",
    "print(\"üß™ Verifying installations...\")\n",
    "try:\n",
    "    import transformers\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import albumentations\n",
    "    \n",
    "    print(f\"‚úÖ Transformers: {transformers.__version__}\")\n",
    "    print(f\"‚úÖ OpenCV: {cv2.__version__}\")\n",
    "    print(f\"‚úÖ NumPy: {np.__version__}\")\n",
    "    print(f\"‚úÖ Albumentations: {albumentations.__version__}\")\n",
    "    \n",
    "    # Verify CV_8U attribute exists\n",
    "    assert hasattr(cv2, 'CV_8U'), \"OpenCV missing CV_8U attribute!\"\n",
    "    print(f\"‚úÖ OpenCV CV_8U attribute: OK\")\n",
    "    \n",
    "    # Verify NumPy version\n",
    "    if np.__version__.startswith('1.'):\n",
    "        print(\"‚úÖ NumPy 1.x confirmed (required for compatibility)\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è WARNING: NumPy {np.__version__} detected, should be 1.x\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All packages verified and working!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Please RESTART the kernel and run this cell again!\")\n",
    "    raise\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå Compatibility error: {e}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Please RESTART the kernel and run this cell again!\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ffe91",
   "metadata": {},
   "source": [
    "### Step 3: Setup Project Structure\n",
    "\n",
    "**üìÅ Kaggle File System:**\n",
    "- `/kaggle/input/` - Read-only input datasets (uploaded by you)\n",
    "- `/kaggle/working/` - Read-write workspace (lost after session)\n",
    "- Results saved here will be available in \"Output\" tab after session ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a692cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project structure in working directory\n",
    "print(\"üìÅ Setting up project structure...\\n\")\n",
    "\n",
    "# Create directories\n",
    "directories = [\n",
    "    'models',\n",
    "    'data',\n",
    "    'scripts',\n",
    "    'utils',\n",
    "    'checkpoints',\n",
    "    'logs',\n",
    "    'results'\n",
    "]\n",
    "\n",
    "for dir_name in directories:\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    print(f\"‚úÖ Created: {dir_name}/\")\n",
    "\n",
    "print(\"\\nüìÇ Kaggle Directory Structure:\")\n",
    "print(\"\\n/kaggle/input/ (Read-Only):\")\n",
    "print(\"  ‚îî‚îÄ‚îÄ Your uploaded datasets appear here\")\n",
    "print(\"\\n/kaggle/working/ (Read-Write):\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ models/\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ data/\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ scripts/\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ utils/\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ checkpoints/  ‚Üê Models saved here\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ logs/         ‚Üê Training logs\")\n",
    "print(\"  ‚îî‚îÄ‚îÄ results/      ‚Üê Test results\")\n",
    "\n",
    "print(\"\\n‚úÖ Project structure created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a17ea8",
   "metadata": {},
   "source": [
    "### Step 4: Copy Project Files from Input\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Upload Your Datasets First!**\n",
    "\n",
    "Before running this cell:\n",
    "\n",
    "1. **Upload Project Code to Kaggle:**\n",
    "   - Go to: https://www.kaggle.com/datasets\n",
    "   - Click \"New Dataset\"\n",
    "   - Upload your `VISIHEALTH CODE` folder (can ZIP it first)\n",
    "   - Suggested name: `visihealth-code`\n",
    "   - Click \"Create\"\n",
    "\n",
    "2. **Upload SLAKE Dataset to Kaggle:**\n",
    "   - Go to: https://www.kaggle.com/datasets  \n",
    "   - Click \"New Dataset\"\n",
    "   - Upload your `Slake1.0` folder (can ZIP it first)\n",
    "   - Suggested name: `slake-medical-vqa` or `my-slake-dataset`\n",
    "   - Click \"Create\"\n",
    "\n",
    "3. **Add Both Datasets to This Notebook:**\n",
    "   - In this notebook, click \"+ Add Data\" (right sidebar ‚Üí Input tab)\n",
    "   - Click \"Your Datasets\" tab\n",
    "   - Add both datasets you just uploaded\n",
    "\n",
    "4. **Update the paths below** to match YOUR dataset names exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: UPDATE THESE PATHS to match YOUR uploaded dataset names!\n",
    "# After uploading, check \"Available input datasets\" below to see exact names\n",
    "PROJECT_INPUT = '/kaggle/input/visihealth-code/VISIHEALTH CODE'      # Your project code dataset\n",
    "SLAKE_INPUT = '/kaggle/input/slake-medical-vqa/Slake1.0'      # Your SLAKE dataset\n",
    "\n",
    "# Examples of what paths might look like:\n",
    "# PROJECT_INPUT = '/kaggle/input/my-visihealth-code'\n",
    "# SLAKE_INPUT = '/kaggle/input/my-slake-dataset'\n",
    "# SLAKE_INPUT = '/kaggle/input/slake1-0'\n",
    "\n",
    "print(\"üîç Checking input datasets...\\n\")\n",
    "\n",
    "# Check what's available in /kaggle/input/\n",
    "print(\"Available input datasets:\")\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    datasets = os.listdir('/kaggle/input')\n",
    "    for dataset in datasets:\n",
    "        print(f\"  üìÅ {dataset}\")\n",
    "        # Show contents\n",
    "        dataset_path = os.path.join('/kaggle/input', dataset)\n",
    "        if os.path.isdir(dataset_path):\n",
    "            contents = os.listdir(dataset_path)[:5]  # First 5 items\n",
    "            for item in contents:\n",
    "                print(f\"      - {item}\")\n",
    "            if len(os.listdir(dataset_path)) > 5:\n",
    "                print(f\"      ... and {len(os.listdir(dataset_path)) - 5} more\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è No datasets found!\")\n",
    "    print(\"\\nüëÜ You need to add datasets using '+ Add Data' button above\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Check if project files exist\n",
    "if os.path.exists(PROJECT_INPUT):\n",
    "    print(f\"‚úÖ Found project at: {PROJECT_INPUT}\")\n",
    "    \n",
    "    # Copy project files\n",
    "    print(\"\\nüì¶ Copying project files...\")\n",
    "    \n",
    "    # Copy models\n",
    "    if os.path.exists(f\"{PROJECT_INPUT}/models\"):\n",
    "        shutil.copytree(f\"{PROJECT_INPUT}/models\", \"models\", dirs_exist_ok=True)\n",
    "        print(\"  ‚úÖ Copied models/\")\n",
    "    \n",
    "    # Copy scripts\n",
    "    if os.path.exists(f\"{PROJECT_INPUT}/scripts\"):\n",
    "        shutil.copytree(f\"{PROJECT_INPUT}/scripts\", \"scripts\", dirs_exist_ok=True)\n",
    "        print(\"  ‚úÖ Copied scripts/\")\n",
    "    \n",
    "    # Copy utils\n",
    "    if os.path.exists(f\"{PROJECT_INPUT}/utils\"):\n",
    "        shutil.copytree(f\"{PROJECT_INPUT}/utils\", \"utils\", dirs_exist_ok=True)\n",
    "        print(\"  ‚úÖ Copied utils/\")\n",
    "    \n",
    "    # Copy data module\n",
    "    if os.path.exists(f\"{PROJECT_INPUT}/data\"):\n",
    "        shutil.copytree(f\"{PROJECT_INPUT}/data\", \"data\", dirs_exist_ok=True)\n",
    "        print(\"  ‚úÖ Copied data/\")\n",
    "    \n",
    "    # Copy config files\n",
    "    if os.path.exists(f\"{PROJECT_INPUT}/config.yaml\"):\n",
    "        shutil.copy(f\"{PROJECT_INPUT}/config.yaml\", \"config.yaml\")\n",
    "        print(\"  ‚úÖ Copied config.yaml\")\n",
    "    \n",
    "    if os.path.exists(f\"{PROJECT_INPUT}/requirements.txt\"):\n",
    "        shutil.copy(f\"{PROJECT_INPUT}/requirements.txt\", \"requirements.txt\")\n",
    "        print(\"  ‚úÖ Copied requirements.txt\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Project files copied!\")\n",
    "else:\n",
    "    print(f\"‚ùå Project not found at: {PROJECT_INPUT}\")\n",
    "    print(\"\\nüìã TO FIX:\")\n",
    "    print(\"1. Click '+ Add Data' button above\")\n",
    "    print(\"2. Go to 'Your Datasets'\")\n",
    "    print(\"3. Upload your VISIHEALTH CODE folder\")\n",
    "    print(\"4. Update PROJECT_INPUT path in this cell\")\n",
    "    print(\"5. Re-run this cell\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Check SLAKE dataset\n",
    "if os.path.exists(SLAKE_INPUT):\n",
    "    print(f\"‚úÖ Found SLAKE dataset at: {SLAKE_INPUT}\")\n",
    "    \n",
    "    # Create symlink or copy (symlink is faster)\n",
    "    print(\"\\nüìä Linking SLAKE dataset...\")\n",
    "    \n",
    "    slake_dest = \"data/SLAKE\"\n",
    "    if os.path.exists(slake_dest):\n",
    "        # Remove existing link or directory\n",
    "        if os.path.islink(slake_dest):\n",
    "            os.unlink(slake_dest)  # Remove symlink\n",
    "        else:\n",
    "            shutil.rmtree(slake_dest)  # Remove directory\n",
    "    \n",
    "    # Create symbolic link (faster than copying)\n",
    "    os.symlink(SLAKE_INPUT, slake_dest)\n",
    "    \n",
    "    # Verify\n",
    "    if os.path.exists(f\"{slake_dest}/train.json\"):\n",
    "        print(\"  ‚úÖ train.json found\")\n",
    "    if os.path.exists(f\"{slake_dest}/test.json\"):\n",
    "        print(\"  ‚úÖ test.json found\")\n",
    "    if os.path.exists(f\"{slake_dest}/imgs\"):\n",
    "        num_imgs = len(os.listdir(f\"{slake_dest}/imgs\"))\n",
    "        print(f\"  ‚úÖ imgs/ found ({num_imgs} images)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ SLAKE dataset linked!\")\n",
    "else:\n",
    "    print(f\"‚ùå SLAKE dataset not found at: {SLAKE_INPUT}\")\n",
    "    print(\"\\nüìã TO FIX:\")\n",
    "    print(\"1. Click '+ Add Data' button above\")\n",
    "    print(\"2. Go to 'Your Datasets'\")\n",
    "    print(\"3. Upload your Slake1.0 folder\")\n",
    "    print(\"4. Update SLAKE_INPUT path in this cell\")\n",
    "    print(\"5. Re-run this cell\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Setup check complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074e94c2",
   "metadata": {},
   "source": [
    "### Step 5: Verify Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d4bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all files are in place\n",
    "print(\"üß™ Verifying project setup...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "core_files = [\n",
    "    'models/cnn_model.py',\n",
    "    'models/bert_model.py',\n",
    "    'models/fusion_model.py',\n",
    "    'models/__init__.py',\n",
    "    'scripts/train.py',\n",
    "    'scripts/demo.py',\n",
    "    'data/dataset.py',\n",
    "    'data/__init__.py',\n",
    "    'utils/knowledge_graph.py',\n",
    "    'utils/__init__.py',\n",
    "    'config.yaml',\n",
    "    'requirements.txt'\n",
    "]\n",
    "\n",
    "print(\"Checking project files:\")\n",
    "all_good = True\n",
    "for file in core_files:\n",
    "    exists = os.path.exists(file)\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"  {status} {file}\")\n",
    "    if not exists:\n",
    "        all_good = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Checking dataset:\")\n",
    "\n",
    "dataset_files = [\n",
    "    'data/SLAKE/train.json',\n",
    "    'data/SLAKE/test.json',\n",
    "    'data/SLAKE/imgs'\n",
    "]\n",
    "\n",
    "for file in dataset_files:\n",
    "    exists = os.path.exists(file)\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"  {status} {file}\")\n",
    "    if not exists:\n",
    "        all_good = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "if all_good:\n",
    "    print(\"‚úÖ SETUP VERIFICATION COMPLETE!\")\n",
    "    print(\"‚úÖ READY TO TRAIN!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SOME FILES ARE MISSING\")\n",
    "    print(\"Go back to Step 4 and check the instructions\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ddd92d",
   "metadata": {},
   "source": [
    "### Step 5.5: Fix Module Imports (Run this if you get ImportError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490bc1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data module imports\n",
    "print(\"üîß Fixing module imports...\\n\")\n",
    "\n",
    "# Ensure data/__init__.py has proper imports\n",
    "data_init_content = \"\"\"\\\"\\\"\\\"Data module for VisiHealth.\\\"\\\"\\\"\n",
    "from .dataset import SLAKEDataset, get_dataloader\n",
    "\n",
    "__all__ = ['SLAKEDataset', 'get_dataloader']\n",
    "\"\"\"\n",
    "\n",
    "with open('data/__init__.py', 'w') as f:\n",
    "    f.write(data_init_content)\n",
    "print(\"‚úÖ Fixed data/__init__.py\")\n",
    "\n",
    "# Ensure models/__init__.py has proper imports\n",
    "models_init_content = \"\"\"\\\"\\\"\\\"Models module for VisiHealth.\\\"\\\"\\\"\n",
    "from .cnn_model import get_cnn_model\n",
    "from .bert_model import get_bert_model\n",
    "from .fusion_model import build_visihealth_model\n",
    "\n",
    "__all__ = ['get_cnn_model', 'get_bert_model', 'build_visihealth_model']\n",
    "\"\"\"\n",
    "\n",
    "with open('models/__init__.py', 'w') as f:\n",
    "    f.write(models_init_content)\n",
    "print(\"‚úÖ Fixed models/__init__.py\")\n",
    "\n",
    "# Ensure utils/__init__.py has proper imports\n",
    "utils_init_content = \"\"\"\\\"\\\"\\\"Utilities module for VisiHealth.\\\"\\\"\\\"\n",
    "from .knowledge_graph import KnowledgeGraph, RationaleGenerator, load_knowledge_graph\n",
    "\n",
    "__all__ = ['KnowledgeGraph', 'RationaleGenerator', 'load_knowledge_graph']\n",
    "\"\"\"\n",
    "\n",
    "with open('utils/__init__.py', 'w') as f:\n",
    "    f.write(utils_init_content)\n",
    "print(\"‚úÖ Fixed utils/__init__.py\")\n",
    "\n",
    "print(\"\\n‚úÖ All module imports fixed! You can now proceed with training.\")\n",
    "print(\"‚ö†Ô∏è If you still get errors, restart the kernel and re-run from Step 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix initialization order bug in train.py\n",
    "print(\"üîß Fixing train.py initialization bug...\\n\")\n",
    "\n",
    "import re\n",
    "\n",
    "# Read train.py\n",
    "with open('scripts/train.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Find the __init__ method and fix the initialization order\n",
    "# The bug: self.class_weights = self._compute_class_weights() is called before self.num_classes is set\n",
    "\n",
    "# Pattern to find where num_classes is set\n",
    "if 'self.num_classes' in content and 'self.class_weights = self._compute_class_weights()' in content:\n",
    "    # We need to move the class_weights line after num_classes is set\n",
    "    # First, remove the problematic line\n",
    "    content_fixed = content.replace(\n",
    "        '        self.class_weights = self._compute_class_weights()',\n",
    "        '        # self.class_weights moved after num_classes is set'\n",
    "    )\n",
    "    \n",
    "    # Find where num_classes is set and add class_weights right after\n",
    "    # Look for the pattern where train_dataset is created and num_classes is set\n",
    "    if 'self.num_classes = self.train_dataset.num_classes' in content_fixed:\n",
    "        content_fixed = content_fixed.replace(\n",
    "            'self.num_classes = self.train_dataset.num_classes',\n",
    "            '''self.num_classes = self.train_dataset.num_classes\n",
    "        \n",
    "        # Compute class weights for balanced training\n",
    "        self.class_weights = self._compute_class_weights()'''\n",
    "        )\n",
    "        \n",
    "        # Write back\n",
    "        with open('scripts/train.py', 'w') as f:\n",
    "            f.write(content_fixed)\n",
    "        \n",
    "        print(\"‚úÖ Fixed train.py initialization order\")\n",
    "        print(\"   Moved class_weights computation after num_classes is set\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Could not find num_classes assignment pattern\")\n",
    "        print(\"   You may need to manually fix train.py\")\n",
    "else:\n",
    "    print(\"‚úÖ train.py appears to be already fixed or has different structure\")\n",
    "\n",
    "print(\"\\n‚úÖ Training script patched!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7911105",
   "metadata": {},
   "source": [
    "### Step 5.6: Fix Training Script Bug (Critical!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d52cbc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è PART 2: TRAINING\n",
    "\n",
    "### Step 6: Load and Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, '/kaggle/working')\n",
    "\n",
    "from data import get_dataloader\n",
    "\n",
    "# Load config\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"üìã Loading SLAKE dataset...\\n\")\n",
    "\n",
    "# Load train dataset\n",
    "train_loader, train_dataset = get_dataloader(\n",
    "    data_dir='data/SLAKE',\n",
    "    split='train',\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    num_workers=2,\n",
    "    tokenizer_name=config['model']['bert']['model_name']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"‚úÖ Number of classes: {train_dataset.num_classes}\")\n",
    "print(f\"‚úÖ Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"‚úÖ Total batches: {len(train_loader)}\")\n",
    "\n",
    "# Visualize samples\n",
    "print(\"\\nüì∏ Visualizing sample data...\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i >= len(sample_batch['image']):\n",
    "        break\n",
    "    \n",
    "    img = sample_batch['image'][i].cpu().numpy().transpose(1, 2, 0)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    question = sample_batch['question_text'][i][:50] + \"...\"\n",
    "    answer = sample_batch['answer_text'][i]\n",
    "    ax.set_title(f\"Q: {question}\\nA: {answer}\", fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Dataset loaded and verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b81a0a0",
   "metadata": {},
   "source": [
    "### Step 7: Start Training\n",
    "\n",
    "**‚è±Ô∏è Training Time Estimates:**\n",
    "- Kaggle GPU T4: ~2-4 hours for 50 epochs\n",
    "- With early stopping: May finish earlier\n",
    "\n",
    "**üíæ Checkpoint Saving:**\n",
    "- Checkpoints saved to `/kaggle/working/checkpoints/`\n",
    "- After session ends, download from \"Output\" tab\n",
    "- Best model automatically saved as `best_checkpoint.pth`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6db179",
   "metadata": {},
   "source": [
    "### Step 6.5: Load Previous Checkpoint (Resume Training)\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Only run this if you want to resume from a previous checkpoint!**\n",
    "\n",
    "If you uploaded your checkpoint as a Kaggle dataset, this will copy it to the working directory so training continues from where it left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# ‚ö†Ô∏è UPDATE THIS PATH to match YOUR uploaded checkpoint dataset name!\n",
    "CHECKPOINT_DATASET = '/kaggle/input/visihealth-checkpoint-epoch45'  # Change this to your dataset name\n",
    "\n",
    "print(\"üì¶ Checking for previous checkpoint to resume training...\\n\")\n",
    "\n",
    "# Check if checkpoint dataset exists\n",
    "if os.path.exists(CHECKPOINT_DATASET):\n",
    "    print(f\"‚úÖ Found checkpoint dataset: {CHECKPOINT_DATASET}\")\n",
    "    \n",
    "    # Look for checkpoint file\n",
    "    checkpoint_file = None\n",
    "    for file in os.listdir(CHECKPOINT_DATASET):\n",
    "        if file.endswith('.pth'):\n",
    "            checkpoint_file = file\n",
    "            break\n",
    "    \n",
    "    if checkpoint_file:\n",
    "        source_path = os.path.join(CHECKPOINT_DATASET, checkpoint_file)\n",
    "        dest_path = f'/kaggle/working/checkpoints/{checkpoint_file}'\n",
    "        \n",
    "        # Create checkpoints directory\n",
    "        os.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n",
    "        \n",
    "        # Copy checkpoint\n",
    "        print(f\"üìã Copying checkpoint: {checkpoint_file}\")\n",
    "        shutil.copy(source_path, dest_path)\n",
    "        \n",
    "        # Verify\n",
    "        size_mb = os.path.getsize(dest_path) / (1024*1024)\n",
    "        print(f\"‚úÖ Checkpoint copied successfully!\")\n",
    "        print(f\"   Location: {dest_path}\")\n",
    "        print(f\"   Size: {size_mb:.1f} MB\")\n",
    "        \n",
    "        # Load and show checkpoint info\n",
    "        import torch\n",
    "        ckpt = torch.load(dest_path, map_location='cpu')\n",
    "        print(f\"\\nüìä Checkpoint Info:\")\n",
    "        print(f\"   Previous Epoch: {ckpt.get('epoch', 'N/A')}\")\n",
    "        print(f\"   Best Val Accuracy: {ckpt.get('best_val_acc', 0):.2f}%\")\n",
    "        print(f\"   Best Val Loss: {ckpt.get('best_val_loss', 0):.4f}\")\n",
    "        print(f\"\\nüîÑ Training will resume from epoch {ckpt.get('epoch', 0) + 1}\")\n",
    "    else:\n",
    "        print(\"‚ùå No .pth file found in checkpoint dataset!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No checkpoint dataset found at: {CHECKPOINT_DATASET}\")\n",
    "    print(\"   Training will start from scratch (epoch 0)\")\n",
    "    print(\"\\nüìã To resume training:\")\n",
    "    print(\"   1. Upload your checkpoint as a Kaggle dataset\")\n",
    "    print(\"   2. Add it to this notebook (+ Add Data)\")\n",
    "    print(\"   3. Update CHECKPOINT_DATASET path in this cell\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea61884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üèãÔ∏è Starting training...\\n\")\n",
    "print(\"Training configuration:\")\n",
    "print(\"  - Auto-resume from latest checkpoint if available\")\n",
    "print(\"  - Train for up to 100 epochs (with early stopping)\")\n",
    "print(\"  - Save checkpoints every 5 epochs\")\n",
    "print(\"  - Log metrics for TensorBoard\")\n",
    "print(\"  - Show progress bars for each epoch\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Change to working directory\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "# Check if checkpoint exists and pass explicit path\n",
    "checkpoint_path = '/kaggle/working/checkpoints/best_checkpoint.pth'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"‚úÖ Found checkpoint: {checkpoint_path}\")\n",
    "    print(\"   Training will resume from this checkpoint\\n\")\n",
    "    !python scripts/train.py --resume --checkpoint {checkpoint_path}\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No checkpoint found - starting from scratch\\n\")\n",
    "    !python scripts/train.py\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd0d92",
   "metadata": {},
   "source": [
    "### Step 8: Monitor Training with TensorBoard (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c3993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard to visualize training metrics\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b403f",
   "metadata": {},
   "source": [
    "### Step 9: Check Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "\n",
    "# List checkpoints\n",
    "checkpoints = glob.glob('checkpoints/*.pth')\n",
    "print(f\"üìä Found {len(checkpoints)} checkpoint(s):\")\n",
    "for ckpt in sorted(checkpoints):\n",
    "    size_mb = os.path.getsize(ckpt) / (1024*1024)\n",
    "    print(f\"  - {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "\n",
    "# Check best checkpoint\n",
    "best_ckpt = 'checkpoints/best_checkpoint.pth'\n",
    "if os.path.exists(best_ckpt):\n",
    "    print(\"\\n‚úÖ Best checkpoint found!\")\n",
    "    \n",
    "    # Load checkpoint info\n",
    "    checkpoint = torch.load(best_ckpt, map_location='cpu')\n",
    "    print(f\"\\nBest model metrics:\")\n",
    "    print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"  Best Val Accuracy: {checkpoint.get('best_val_acc', 0):.2f}%\")\n",
    "    print(f\"  Best Val Loss: {checkpoint.get('best_val_loss', 0):.4f}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Checkpoint size: {os.path.getsize(best_ckpt) / (1024*1024):.1f} MB\")\n",
    "    print(f\"üìÅ Location: {os.path.abspath(best_ckpt)}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No best checkpoint found yet.\")\n",
    "    print(\"Training may still be in progress or hasn't completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a64f93b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÆ PART 3: INFERENCE & EVALUATION\n",
    "\n",
    "### Step 10: Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from models import get_cnn_model, get_bert_model, build_visihealth_model\n",
    "from data.dataset import SLAKEDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load config\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load checkpoint FIRST to get the training vocabulary\n",
    "CHECKPOINT_PATH = 'checkpoints/best_checkpoint.pth'\n",
    "print(f\"üì¶ Loading checkpoint from: {CHECKPOINT_PATH}\")\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "\n",
    "# Get answer vocabulary from checkpoint or training data\n",
    "if 'answer_vocab' in checkpoint:\n",
    "    answer_vocab = checkpoint['answer_vocab']\n",
    "    num_classes = len(answer_vocab)\n",
    "    print(f\"‚úÖ Loaded answer vocabulary from checkpoint: {num_classes} classes\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No answer vocab in checkpoint, loading from train dataset...\")\n",
    "    # Import get_dataloader to load train dataset\n",
    "    from data import get_dataloader\n",
    "    _, train_dataset = get_dataloader(\n",
    "        data_dir='data/SLAKE',\n",
    "        split='train',\n",
    "        batch_size=1,\n",
    "        num_workers=2,\n",
    "        tokenizer_name=config['model']['bert']['model_name']\n",
    "    )\n",
    "    answer_vocab = train_dataset.answer_vocab\n",
    "    num_classes = train_dataset.num_classes\n",
    "    print(f\"‚úÖ Loaded answer vocabulary from train dataset: {num_classes} classes\")\n",
    "\n",
    "# Load test dataset with the SAME vocabulary as training\n",
    "print(\"\\nüìä Loading test dataset with training vocabulary...\")\n",
    "\n",
    "test_dataset = SLAKEDataset(\n",
    "    data_dir='data/SLAKE',\n",
    "    split='test',\n",
    "    tokenizer_name=config['model']['bert']['model_name'],\n",
    "    answer_vocab=answer_vocab,  # Use training vocabulary!\n",
    "    max_length=config['model']['bert']['max_length']\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Test dataset: {len(test_dataset)} samples\")\n",
    "print(f\"‚úÖ Using {num_classes} answer classes (from training)\")\n",
    "\n",
    "# Build model with correct number of classes\n",
    "print(\"\\nüß† Building model...\")\n",
    "\n",
    "# Update config with correct num_classes in the EXACT location the model reads from\n",
    "config['num_classes'] = num_classes\n",
    "config['model']['num_classes'] = num_classes\n",
    "config['model']['cnn']['num_classes'] = num_classes  # This is the key one!\n",
    "if 'fusion' not in config:\n",
    "    config['fusion'] = {}\n",
    "config['fusion']['num_classes'] = num_classes\n",
    "\n",
    "print(f\"‚öôÔ∏è Building model with {num_classes} answer classes...\")\n",
    "print(f\"üîç Config model.cnn.num_classes: {config['model']['cnn'].get('num_classes', 'NOT SET')}\")\n",
    "\n",
    "cnn = get_cnn_model(config)\n",
    "bert = get_bert_model(config)\n",
    "\n",
    "# Build model - it reads num_classes from config['model']['cnn']['num_classes']\n",
    "model = build_visihealth_model(config, cnn, bert)\n",
    "\n",
    "# Load checkpoint weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Trained for {checkpoint.get('epoch', 'N/A')} epochs\")\n",
    "print(f\"   Best validation accuracy: {checkpoint.get('best_val_acc', 0):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b0645",
   "metadata": {},
   "source": [
    "### Step 11: Run Inference on Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Get random samples\n",
    "num_samples = 6\n",
    "indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n",
    "\n",
    "correct_predictions = 0\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flat\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    sample = test_dataset[idx]\n",
    "    \n",
    "    # Prepare input\n",
    "    image = sample['image'].unsqueeze(0).to(device)\n",
    "    input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "    attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image, input_ids, attention_mask, return_attention=True)\n",
    "        answer_logits = outputs['answer_logits']\n",
    "        roi_scores = outputs['roi_scores']\n",
    "    \n",
    "    # Get predictions\n",
    "    answer_probs = F.softmax(answer_logits, dim=1)\n",
    "    pred_idx = answer_logits.argmax(dim=1).item()\n",
    "    confidence = answer_probs[0, pred_idx].item()\n",
    "    \n",
    "    pred_answer = test_dataset.get_answer_text(pred_idx)\n",
    "    true_answer = sample['answer_text']\n",
    "    \n",
    "    is_correct = pred_answer.lower() == true_answer.lower()\n",
    "    if is_correct:\n",
    "        correct_predictions += 1\n",
    "    \n",
    "    # Get top ROI\n",
    "    top_roi_idx = roi_scores.argmax(dim=1).item()\n",
    "    roi_confidence = F.softmax(roi_scores, dim=1)[0, top_roi_idx].item()\n",
    "    \n",
    "    # Visualize\n",
    "    img_display = sample['image'].cpu().numpy().transpose(1, 2, 0)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img_display = std * img_display + mean\n",
    "    img_display = np.clip(img_display, 0, 1)\n",
    "    \n",
    "    axes[i].imshow(img_display)\n",
    "    \n",
    "    status = \"‚úÖ CORRECT\" if is_correct else \"‚ùå WRONG\"\n",
    "    title = (\n",
    "        f\"Q: {sample['question_text'][:40]}...\\n\"\n",
    "        f\"Pred: {pred_answer} ({confidence:.1%}) | True: {true_answer}\\n\"\n",
    "        f\"{status} | ROI: {top_roi_idx} ({roi_confidence:.1%})\"\n",
    "    )\n",
    "    axes[i].set_title(title, fontsize=8)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Accuracy on these samples: {correct_predictions}/{num_samples} ({100*correct_predictions/num_samples:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7667260b",
   "metadata": {},
   "source": [
    "### Step 12: Generate Rationales with Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183dab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check if KG CSV files exist in the SLAKE dataset\n",
    "kg_dir = 'data/SLAKE/KG'\n",
    "kg_output_file = 'kg.txt'  # Write to writable /kaggle/working/ directory\n",
    "\n",
    "if os.path.exists(kg_dir):\n",
    "    print(f\"üîç Found KG directory: {kg_dir}\")\n",
    "    print(\"üìä Converting CSV files to kg.txt...\\n\")\n",
    "    \n",
    "    # List of CSV files to process\n",
    "    csv_files = [\n",
    "        'en_disease.csv',\n",
    "        'en_organ.csv', \n",
    "        'en_organ_rel.csv'\n",
    "    ]\n",
    "    \n",
    "    all_triplets = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(kg_dir, csv_file)\n",
    "        if os.path.exists(csv_path):\n",
    "            print(f\"  üìÑ Reading {csv_file}...\")\n",
    "            df = pd.read_csv(csv_path, sep='#', header=0, names=['entity', 'relation', 'value'])\n",
    "            \n",
    "            # Convert to triplets (entity, relation, value)\n",
    "            for _, row in df.iterrows():\n",
    "                entity = str(row['entity']).strip().lower()\n",
    "                relation = str(row['relation']).strip().replace(' ', '_').lower()\n",
    "                value = str(row['value']).strip().lower()\n",
    "                \n",
    "                # Skip header rows or invalid data\n",
    "                if entity == 'organ' or entity == 'nan':\n",
    "                    continue\n",
    "                \n",
    "                # For multi-value entries (comma-separated), create separate triplets\n",
    "                if ',' in value:\n",
    "                    values = [v.strip() for v in value.split(',')]\n",
    "                    for v in values:\n",
    "                        if v and v != 'nan':\n",
    "                            all_triplets.append(f\"{entity}\\t{relation}\\t{v}\")\n",
    "                else:\n",
    "                    if value and value != 'nan':\n",
    "                        all_triplets.append(f\"{entity}\\t{relation}\\t{value}\")\n",
    "            \n",
    "            print(f\"    ‚úÖ Extracted {len(all_triplets)} triplets so far\")\n",
    "    \n",
    "    # Remove duplicates and write to kg.txt in writable location\n",
    "    all_triplets = list(set(all_triplets))\n",
    "    all_triplets.sort()\n",
    "    \n",
    "    with open(kg_output_file, 'w', encoding='utf-8') as f:\n",
    "        for triplet in all_triplets:\n",
    "            f.write(triplet + '\\n')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created {kg_output_file} with {len(all_triplets)} unique triplets!\")\n",
    "    print(f\"üìÅ Location: {os.path.abspath(kg_output_file)}\")\n",
    "    print(f\"   (Saved to writable /kaggle/working/ directory)\")\n",
    "    \n",
    "    # Show sample triplets\n",
    "    print(\"\\nüìã Sample triplets:\")\n",
    "    for triplet in all_triplets[:10]:\n",
    "        parts = triplet.split('\\t')\n",
    "        print(f\"  ‚Ä¢ {parts[0]} ‚Üí {parts[1]} ‚Üí {parts[2]}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è KG directory not found at: {kg_dir}\")\n",
    "    print(\"   This step is optional. A sample KG will be created in Step 12 if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec894cef",
   "metadata": {},
   "source": [
    "### Step 11.5: Convert KG CSV Files to kg.txt (Optional Enhancement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.knowledge_graph import load_knowledge_graph, RationaleGenerator\n",
    "\n",
    "# Load KG - check read-only input first, then create in writable location if needed\n",
    "kg_file_readonly = 'data/SLAKE/kg.txt'  # Read-only location\n",
    "kg_file_writable = 'kg.txt'  # Writable location in /kaggle/working\n",
    "\n",
    "if os.path.exists(kg_file_readonly):\n",
    "    kg_file = kg_file_readonly\n",
    "    kg = load_knowledge_graph(kg_file)\n",
    "    rationale_gen = RationaleGenerator(kg)\n",
    "    print(f\"‚úÖ Knowledge graph loaded from dataset: {len(kg.triplets)} triplets\")\n",
    "elif os.path.exists(kg_file_writable):\n",
    "    kg_file = kg_file_writable\n",
    "    kg = load_knowledge_graph(kg_file)\n",
    "    rationale_gen = RationaleGenerator(kg)\n",
    "    print(f\"‚úÖ Knowledge graph loaded: {len(kg.triplets)} triplets\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è KG file not found. Creating sample knowledge graph...\")\n",
    "    kg_file = kg_file_writable\n",
    "    with open(kg_file, 'w') as f:\n",
    "        f.write(\"liver\\tis_located_in\\tabdomen\\n\")\n",
    "        f.write(\"lung\\tis_located_in\\tchest\\n\")\n",
    "        f.write(\"heart\\tis_located_in\\tchest\\n\")\n",
    "        f.write(\"brain\\tis_located_in\\thead\\n\")\n",
    "        f.write(\"kidney\\tis_located_in\\tabdomen\\n\")\n",
    "    kg = load_knowledge_graph(kg_file)\n",
    "    rationale_gen = RationaleGenerator(kg)\n",
    "    print(f\"‚úÖ Created sample knowledge graph: {len(kg.triplets)} triplets\")\n",
    "\n",
    "# Generate rationales for first 3 samples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING RATIONALES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx in indices[:3]:\n",
    "    sample = test_dataset[idx]\n",
    "    \n",
    "    # Prepare input\n",
    "    image = sample['image'].unsqueeze(0).to(device)\n",
    "    input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "    attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image, input_ids, attention_mask)\n",
    "        answer_logits = outputs['answer_logits']\n",
    "        roi_scores = outputs['roi_scores']\n",
    "    \n",
    "    # Get predictions\n",
    "    answer_probs = F.softmax(answer_logits, dim=1)\n",
    "    pred_idx = answer_logits.argmax(dim=1).item()\n",
    "    confidence = answer_probs[0, pred_idx].item()\n",
    "    pred_answer = test_dataset.get_answer_text(pred_idx)\n",
    "    \n",
    "    # Get top ROIs\n",
    "    top_k_rois = torch.topk(roi_scores[0], k=3)\n",
    "    \n",
    "    # Generate rationale\n",
    "    rationale = rationale_gen.generate_rationale(\n",
    "        predicted_answer=pred_answer,\n",
    "        confidence=confidence,\n",
    "        top_roi_indices=top_k_rois.indices.tolist(),\n",
    "        roi_scores=top_k_rois.values.tolist(),\n",
    "        question=sample['question_text']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Image: {sample['img_name']}\")\n",
    "    print(f\"Question: {sample['question_text']}\")\n",
    "    print(f\"True Answer: {sample['answer_text']}\")\n",
    "    print(f\"Predicted Answer: {pred_answer} (confidence: {confidence:.2%})\")\n",
    "    print(f\"\\nüìù Rationale:\\n{rationale}\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91a0b6",
   "metadata": {},
   "source": [
    "### Step 13: Calculate Full Test Set Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3900302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Evaluate on entire test set\n",
    "print(\"üìä Evaluating on full test set...\\n\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Create test dataset with SAME vocabulary as training (must pass answer_vocab!)\n",
    "test_dataset_batch = SLAKEDataset(\n",
    "    data_dir='data/SLAKE',\n",
    "    split='test',\n",
    "    tokenizer_name=config['model']['bert']['model_name'],\n",
    "    answer_vocab=answer_vocab,  # CRITICAL: Use training vocabulary!\n",
    "    max_length=config['model']['bert']['max_length']\n",
    ")\n",
    "\n",
    "test_loader_batch = DataLoader(\n",
    "    test_dataset_batch,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Test dataset: {len(test_dataset_batch)} samples with {len(answer_vocab)} classes (training vocab)\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_batch, desc=\"Evaluating\"):\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        answers = batch['answer'].to(device)\n",
    "        \n",
    "        outputs = model(images, input_ids, attention_mask)\n",
    "        predictions = outputs['answer_logits'].argmax(dim=1)\n",
    "        \n",
    "        correct += (predictions == answers).sum().item()\n",
    "        total += answers.size(0)\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä TEST SET RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total Samples: {total}\")\n",
    "print(f\"Correct Predictions: {correct}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'test_accuracy': accuracy,\n",
    "    'correct': correct,\n",
    "    'total': total,\n",
    "    'checkpoint': CHECKPOINT_PATH,\n",
    "    'platform': 'Kaggle'\n",
    "}\n",
    "\n",
    "import json\n",
    "results_file = 'results/VisiHealth_Results.json'\n",
    "os.makedirs('results', exist_ok=True)\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca38887",
   "metadata": {},
   "source": [
    "### Step 13.5: Verify Files Are Saved\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL: How Kaggle Output Works**\n",
    "\n",
    "Files in `/kaggle/working` are ONLY saved to Output if:\n",
    "1. The notebook runs ALL cells to completion\n",
    "2. OR you click **\"Save & Run All (Commit)\"** button\n",
    "\n",
    "DO NOT manually click \"Save Version\" while running - files will be lost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"üîç Checking saved model files in /kaggle/working...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check checkpoints folder\n",
    "checkpoint_dir = '/kaggle/working/checkpoints'\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = glob.glob(f'{checkpoint_dir}/*.pth')\n",
    "    print(f\"\\n‚úÖ CHECKPOINTS FOLDER EXISTS:\")\n",
    "    print(f\"   Location: {checkpoint_dir}\")\n",
    "    print(f\"   Files found: {len(checkpoints)}\")\n",
    "    for ckpt in checkpoints:\n",
    "        size_mb = os.path.getsize(ckpt) / (1024*1024)\n",
    "        print(f\"   ‚Ä¢ {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå CHECKPOINTS FOLDER NOT FOUND!\")\n",
    "    print(f\"   Expected at: {checkpoint_dir}\")\n",
    "\n",
    "# Check results folder\n",
    "results_dir = '/kaggle/working/results'\n",
    "if os.path.exists(results_dir):\n",
    "    results = glob.glob(f'{results_dir}/*.json')\n",
    "    print(f\"\\n‚úÖ RESULTS FOLDER EXISTS:\")\n",
    "    print(f\"   Location: {results_dir}\")\n",
    "    print(f\"   Files found: {len(results)}\")\n",
    "    for res in results:\n",
    "        size_kb = os.path.getsize(res) / 1024\n",
    "        print(f\"   ‚Ä¢ {os.path.basename(res)} ({size_kb:.1f} KB)\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå RESULTS FOLDER NOT FOUND!\")\n",
    "    print(f\"   Expected at: {results_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìã IMPORTANT: HOW TO SAVE FILES TO KAGGLE OUTPUT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüö® METHOD 1 (RECOMMENDED):\")\n",
    "print(\"   1. Click: 'Save & Run All (Commit)' button (top right)\")\n",
    "print(\"   2. Wait for ALL cells to complete\")\n",
    "print(\"   3. Go to 'Versions' tab ‚Üí Find completed version\")\n",
    "print(\"   4. Click on version ‚Üí Go to 'Output' tab\")\n",
    "print(\"   5. Download 'checkpoints' folder (contains models)\")\n",
    "\n",
    "print(\"\\nüö® METHOD 2 (If session is active):\")\n",
    "print(\"   1. Let notebook run to the VERY LAST cell\")\n",
    "print(\"   2. After last cell finishes, files save automatically\")\n",
    "print(\"   3. Session ends ‚Üí Go to 'Output' tab\")\n",
    "print(\"   4. Download files\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  DO NOT:\")\n",
    "print(\"   ‚ùå Click 'Save Version' while notebook is running\")\n",
    "print(\"   ‚ùå Stop session before notebook completes\")\n",
    "print(\"   ‚ùå These will cause files to be LOST!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Files are ready in /kaggle/working/\")\n",
    "print(\"   They will appear in Output tab AFTER notebook completes!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üì¶ FINAL KAGGLE DOWNLOAD CELL\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "ZIP_NAME = \"VisiHealth_Training_Output.zip\"\n",
    "ZIP_PATH = f\"/kaggle/working/{ZIP_NAME}\"\n",
    "\n",
    "folders_to_zip = {\n",
    "    \"checkpoints\": \"/kaggle/working/checkpoints\",\n",
    "    \"results\": \"/kaggle/working/results\"\n",
    "}\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for folder_name, folder_path in folders_to_zip.items():\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"‚ö†Ô∏è Skipping missing folder: {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.join(folder_name, file)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "print(\"‚úÖ ZIP file created successfully!\")\n",
    "display(FileLink(ZIP_PATH))\n",
    "print(\"üì• You can now download the ZIP or get it from the Output tab.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd312343",
   "metadata": {},
   "source": [
    "### Step 14: Export Model Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model info for frontend integration\n",
    "export_info = {\n",
    "    'model_type': 'VisiHealth AI',\n",
    "    'checkpoint_path': CHECKPOINT_PATH,\n",
    "    'test_accuracy': accuracy,\n",
    "    'num_classes': test_dataset.num_classes,\n",
    "    'answer_vocab': {v: k for k, v in test_dataset.answer_vocab.items()},\n",
    "    'image_size': 224,\n",
    "    'bert_model': config['model']['bert']['model_name'],\n",
    "    'usage': {\n",
    "        'input': 'Medical image (224x224) + question text',\n",
    "        'output': 'Answer + confidence + ROI scores + rationale'\n",
    "    },\n",
    "    'training_info': {\n",
    "        'dataset': 'SLAKE 1.0',\n",
    "        'total_samples': len(train_dataset),\n",
    "        'epochs': checkpoint.get('epoch', 'N/A'),\n",
    "        'best_val_acc': checkpoint.get('best_val_acc', 0),\n",
    "        'platform': 'Kaggle'\n",
    "    }\n",
    "}\n",
    "\n",
    "info_file = 'results/VisiHealth_Model_Info.json'\n",
    "with open(info_file, 'w') as f:\n",
    "    json.dump(export_info, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model info exported to: {info_file}\")\n",
    "print(\"\\nThis file contains:\")\n",
    "print(\"  - Model checkpoint path\")\n",
    "print(\"  - Test accuracy\")\n",
    "print(\"  - Answer vocabulary (for mapping predictions)\")\n",
    "print(\"  - Input/output specifications\")\n",
    "print(\"  - Training information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404ae66",
   "metadata": {},
   "source": [
    "### Step 15: Prepare Files for Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of all important files to download\n",
    "print(\"üì¶ Preparing files for download...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "download_files = {\n",
    "    'Checkpoints': glob.glob('checkpoints/*.pth'),\n",
    "    'Results': glob.glob('results/*.json'),\n",
    "    'Logs': ['logs/'] if os.path.exists('logs/') else []\n",
    "}\n",
    "\n",
    "total_size = 0\n",
    "\n",
    "for category, files in download_files.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    if not files:\n",
    "        print(\"  (none)\")\n",
    "        continue\n",
    "    \n",
    "    for file in files:\n",
    "        if os.path.isfile(file):\n",
    "            size_mb = os.path.getsize(file) / (1024*1024)\n",
    "            total_size += size_mb\n",
    "            print(f\"  ‚úÖ {file} ({size_mb:.1f} MB)\")\n",
    "        elif os.path.isdir(file):\n",
    "            dir_size = sum(\n",
    "                os.path.getsize(os.path.join(dirpath, filename))\n",
    "                for dirpath, dirnames, filenames in os.walk(file)\n",
    "                for filename in filenames\n",
    "            ) / (1024*1024)\n",
    "            total_size += dir_size\n",
    "            print(f\"  ‚úÖ {file} ({dir_size:.1f} MB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"üìä Total size: {total_size:.1f} MB\")\n",
    "print(\"\\nüíæ After session ends, download from:\")\n",
    "print(\"   1. Click 'Output' tab (top right)\")\n",
    "print(\"   2. Download all files\")\n",
    "print(\"   3. Extract and use locally\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce26c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ TRAINING COMPLETE!\n",
    "\n",
    "### üéâ Congratulations! Your Model is Trained!\n",
    "\n",
    "### üìÅ What You Have Now:\n",
    "\n",
    "**In Kaggle Output (Download after session ends):**\n",
    "- ‚úÖ `checkpoints/best_checkpoint.pth` - Your trained model (~500MB)\n",
    "- ‚úÖ `checkpoints/checkpoint_epoch_XX.pth` - Training checkpoints\n",
    "- ‚úÖ `results/VisiHealth_Results.json` - Test accuracy and metrics\n",
    "- ‚úÖ `results/VisiHealth_Model_Info.json` - Model specifications\n",
    "- ‚úÖ `logs/` - TensorBoard training logs\n",
    "\n",
    "### üì• How to Download Your Files:\n",
    "\n",
    "1. **Wait for session to end** or click \"Stop Session\"\n",
    "2. **Click \"Output\" tab** (top right of notebook)\n",
    "3. **Download all files** - especially the checkpoints folder\n",
    "4. **Extract on your local machine**\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "#### Option 1: Use Locally\n",
    "```bash\n",
    "# On your laptop\n",
    "python scripts/demo.py --checkpoint checkpoints/best_checkpoint.pth\n",
    "```\n",
    "\n",
    "#### Option 2: Deploy to Web\n",
    "- Use Flask/FastAPI for backend\n",
    "- Load checkpoint in API endpoint\n",
    "- Build React/Vue frontend\n",
    "- Deploy on Heroku/AWS/Azure\n",
    "\n",
    "#### Option 3: Continue Training\n",
    "- Add this notebook to a new Kaggle session\n",
    "- Add your checkpoint as a dataset\n",
    "- Resume training with `--resume` flag\n",
    "\n",
    "### üìä Performance Metrics:\n",
    "- **Training Platform:** Kaggle (GPU T4)\n",
    "- **Dataset:** SLAKE 1.0\n",
    "- **Test Accuracy:** See Step 13 results above\n",
    "- **Model Size:** ~500 MB\n",
    "- **Inference Speed:** ~200-300ms per image (GPU)\n",
    "\n",
    "### üí° Tips:\n",
    "- Keep your checkpoint file safe - it contains all your training!\n",
    "- The answer vocabulary is in Model_Info.json - you need this for predictions\n",
    "- Test locally before deploying to ensure everything works\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ You're All Set!\n",
    "\n",
    "Your medical VQA system is trained and ready to use. Download your files and start building amazing applications!\n",
    "\n",
    "**Questions?** Check the project documentation or experiment with the demo script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab4582",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è FINAL SAFETY CHECK - READ THIS BEFORE STOPPING!\n",
    "\n",
    "**üö® CRITICAL: If you're seeing this, DON'T STOP THE SESSION YET! üö®**\n",
    "\n",
    "Run the cell below to verify your files will be saved to Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d66c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîç FINAL FILE VERIFICATION - DO NOT SKIP THIS!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if training completed\n",
    "checkpoint_dir = '/kaggle/working/checkpoints'\n",
    "results_dir = '/kaggle/working/results'\n",
    "\n",
    "checkpoints = glob.glob(f'{checkpoint_dir}/*.pth') if os.path.exists(checkpoint_dir) else []\n",
    "results = glob.glob(f'{results_dir}/*.json') if os.path.exists(results_dir) else []\n",
    "\n",
    "print(\"\\nüìä FILES CURRENTLY IN /kaggle/working/:\\n\")\n",
    "\n",
    "if checkpoints:\n",
    "    print(\"‚úÖ CHECKPOINTS FOUND:\")\n",
    "    for ckpt in checkpoints:\n",
    "        size_mb = os.path.getsize(ckpt) / (1024*1024)\n",
    "        print(f\"   ‚Ä¢ {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"‚ùå NO CHECKPOINTS FOUND!\")\n",
    "    print(\"   Training may have failed!\")\n",
    "\n",
    "if results:\n",
    "    print(\"\\n‚úÖ RESULTS FOUND:\")\n",
    "    for res in results:\n",
    "        size_kb = os.path.getsize(res) / 1024\n",
    "        print(f\"   ‚Ä¢ {os.path.basename(res)} ({size_kb:.1f} KB)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå NO RESULTS FOUND!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üö® CRITICAL INSTRUCTIONS - READ CAREFULLY:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if checkpoints and results:\n",
    "    print(\"\\n‚úÖ YOUR FILES EXIST IN /kaggle/working/\")\n",
    "    print(\"\\nüìã HOW TO SAVE THEM TO OUTPUT TAB:\")\n",
    "    print(\"\\n   IF YOU USED 'Save & Run All (Commit)':\")\n",
    "    print(\"   ‚úÖ This cell is the LAST cell\")\n",
    "    print(\"   ‚úÖ When this finishes, Kaggle will AUTO-SAVE your files\")\n",
    "    print(\"   ‚úÖ Go to: Versions tab ‚Üí Find this version ‚Üí Output tab\")\n",
    "    print(\"   ‚úÖ Download your files from there\")\n",
    "    print(\"\\n   IF YOU MANUALLY RAN CELLS:\")\n",
    "    print(\"   ‚ö†Ô∏è  Your files will be LOST when session ends!\")\n",
    "    print(\"   ‚ö†Ô∏è  You MUST use 'Save & Run All (Commit)' to save them!\")\n",
    "    print(\"   ‚ö†Ô∏è  DO NOT manually stop this session!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ IF THIS IS THE LAST CELL TO RUN:\")\n",
    "    print(\"   Wait 30 seconds after this completes\")\n",
    "    print(\"   Files will auto-save to Output\")\n",
    "    print(\"   Then you can safely close/stop\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n‚ùå FILES ARE MISSING!\")\n",
    "    print(\"\\n   Possible reasons:\")\n",
    "    print(\"   1. Training failed (check earlier cells)\")\n",
    "    print(\"   2. Training didn't run (did you skip Step 7?)\")\n",
    "    print(\"   3. File paths are wrong\")\n",
    "    print(\"\\n   ‚ö†Ô∏è  DO NOT STOP - go back and check training cell!\")\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
