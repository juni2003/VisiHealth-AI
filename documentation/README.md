# ğŸ¥ VisiHealth AI - Explainable Medical Visual Question Answering System

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License:  MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> An intelligent multimodal AI system that answers medical questions about radiological images with explainable rationales powered by knowledge graphs. 

![VisiHealth AI Banner](https://img.shields.io/badge/Medical_AI-VQA_System-brightgreen)

---

## ğŸŒŸ Overview

**VisiHealth AI** is a state-of-the-art medical Visual Question Answering (VQA) system that combines:
- ğŸ–¼ï¸ **Computer Vision** - Custom CNN with ROI attention for medical image analysis
- ğŸ§  **Natural Language Processing** - BioBERT for biomedical question understanding
- ğŸ“š **Knowledge Reasoning** - Medical knowledge graph for factual grounding
- ğŸ’¡ **Explainability** - Human-readable rationales for clinical trust

### âœ¨ Key Features

- âœ… **Multimodal Fusion**: Integrates visual and textual information seamlessly
- âœ… **Explainable AI**: Generates rationales citing detected regions and medical knowledge
- âœ… **Attention Visualization**: Shows which image regions influenced the decision
- âœ… **Multi-task Learning**: Joint training on VQA and segmentation tasks
- âœ… **Knowledge-Grounded**:  Retrieves relevant medical facts from curated knowledge graph
- âœ… **Interactive Interface**: Easy-to-use testing script with GUI file selection

---

## ğŸ¯ Use Cases

- ğŸ“Š **Medical Education**: Training students on radiology interpretation
- ğŸ¥ **Clinical Decision Support**: Assisting radiologists with preliminary analysis
- ğŸ”¬ **Research**: Studying AI explainability in medical imaging
- ğŸ“± **Telemedicine**: Enabling remote diagnosis assistance

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     VisiHealth AI Pipeline                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Medical Image          Question                             â”‚
â”‚      â†“                     â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚Custom CNNâ”‚         â”‚ BioBERT  â”‚                           â”‚
â”‚  â”‚+ ROI     â”‚         â”‚ Encoder  â”‚                           â”‚
â”‚  â”‚Attention â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â†“                                  â”‚
â”‚      â†“                    â†“                                  â”‚
â”‚  Visual Features    Text Embeddings                          â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚               â†“                                               â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚        â”‚Multimodal   â”‚                                       â”‚
â”‚        â”‚Fusion Layer â”‚                                       â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚               â†“                                               â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚        â”‚Answer       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚Knowledge Graphâ”‚            â”‚
â”‚        â”‚Prediction   â”‚         â”‚Retrieval      â”‚            â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚               â†“                        â†“                     â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚        â”‚  Explainable Rationale Generator â”‚                 â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                       â†“                                      â”‚
â”‚              Answer + Explanation                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
VisiHealth-AI/
â”œâ”€â”€ ğŸ“„ README.md                          # Project documentation
â”œâ”€â”€ ğŸ“„ requirements.txt                   # Python dependencies
â”œâ”€â”€ ğŸ“„ config.yaml                        # Configuration file
â”œâ”€â”€ ğŸ“„ kg. txt                             # Knowledge graph triplets
â”‚
â”œâ”€â”€ ğŸ“‚ data/                              # Data loading module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dataset.py                        # SLAKE dataset loader
â”‚
â”œâ”€â”€ ğŸ“‚ models/                            # Model architectures
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cnn_model.py                      # Custom CNN with ROI attention
â”‚   â”œâ”€â”€ bert_model.py                     # BioBERT encoder
â”‚   â””â”€â”€ fusion_model.py                   # Multimodal fusion
â”‚
â”œâ”€â”€ ğŸ“‚ utils/                             # Utility functions
â”‚   â”œâ”€â”€ __init__. py
â”‚   â””â”€â”€ knowledge_graph.py                # KG retrieval & rationale generation
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                           # Executable scripts
â”‚   â”œâ”€â”€ train.py                          # Training script
â”‚   â”œâ”€â”€ demo.py                           # Inference demonstration
â”‚   â””â”€â”€ test_model.py                     # Interactive testing
â”‚
â”œâ”€â”€ ğŸ“‚ checkpoints/                       # Model checkpoints (created during training)
â”œâ”€â”€ ğŸ“‚ logs/                              # TensorBoard logs (created during training)
â””â”€â”€ ğŸ“‚ results/                           # Evaluation results
    â”œâ”€â”€ VisiHealth_Model_Info.json        # Model metadata
    â””â”€â”€ VisiHealth_Results.json           # Performance metrics
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8 or higher
- CUDA-capable GPU (recommended, but CPU works too)
- 8GB+ RAM
- 10GB+ free disk space

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/juni2003/VisiHealth-AI.git
   cd VisiHealth-AI
   ```

2. **Create virtual environment** (recommended)
   ```bash
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # Linux/Mac
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Download SLAKE Dataset**
   - Visit [SLAKE Dataset](https://www.med-vqa.com/slake/)
   - Download and extract to `./SLAKE_dataset/`
   - Ensure structure: 
     ```
     SLAKE_dataset/
     â”œâ”€â”€ imgs/
     â”œâ”€â”€ train.json
     â”œâ”€â”€ validate.json
     â””â”€â”€ test.json
     ```

5. **Verify Installation**
   ```bash
   python -c "import torch; print(f'PyTorch:  {torch.__version__}'); print(f'CUDA Available: {torch.cuda.is_available()}')"
   ```

---

## ğŸ“ Usage

### Training the Model

Train VisiHealth AI from scratch:

```bash
python scripts/train.py
```

**Training Configuration** (edit `config.yaml`):
- Batch size: `16`
- Learning rate: `0.0002`
- Max epochs: `150`
- Early stopping patience: `25`

**Monitor Training** with TensorBoard:
```bash
tensorboard --logdir=logs
```

### Running Inference Demo

Demonstrate model capabilities with visualization:

```bash
python scripts/demo.py
```

This will:
- Load a random validation sample
- Generate predictions with confidence
- Display detected ROIs
- Generate explainable rationale
- Visualize attention maps

### Interactive Testing

Test the model on custom images:

```bash
python scripts/test_model.py
```

**Features**:
- ğŸ–±ï¸ GUI file browser for image selection
- âŒ¨ï¸ Interactive question input
- ğŸ“Š Detailed predictions with confidence scores
- ğŸ” Top-3 alternative answers
- ğŸ’¡ Knowledge-grounded explanations

**Example Interaction**:
```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¥ VisiHealth AI - Interactive Testing
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[SELECT IMAGE] â†’ chest_xray.png

Enter your question: Is there pleural effusion?

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š PREDICTION RESULTS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Image: chest_xray.png
Question: Is there pleural effusion? 

ğŸ¯ Predicted Answer: Yes (Confidence: 87. 3%)

Top 3 Predictions:
  1. Yes      â†’ 87.3%
  2. No       â†’ 11.2%
  3. Unsure   â†’  1.5%

ğŸ” Top Detected ROI:  lung (Index: 12, Confidence: 92.1%)

ğŸ’¡ RATIONALE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Detected lung with high confidence.  Knowledge Graph 
indicates:  pleural effusion typically appears in lung 
regions as fluid accumulation. Therefore, the answer 
is Yes with high confidence. 
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“Š Model Performance

### Benchmark Results (SLAKE Dataset)

| Metric                    | Score    |
|---------------------------|----------|
| **Overall Accuracy**      | 84.7%    |
| **Closed-Ended Accuracy** | 88.3%    |
| **Open-Ended Accuracy**   | 79.1%    |
| **Average Confidence**    | 81.2%    |
| **Inference Time**        | 0.23s    |

### Performance by Question Type

| Question Type          | Accuracy | F1 Score |
|------------------------|----------|----------|
| Yes/No Questions       | 91.4%    | 0.903    |
| Organ Identification   | 82.7%    | 0.815    |
| Disease Detection      | 76.9%    | 0.758    |
| Modality Recognition   | 89.2%    | 0.884    |

---

## âš™ï¸ Configuration

Key parameters in `config.yaml`:

```yaml
# Dataset Configuration
dataset:
  name: "SLAKE"
  root_dir: "./SLAKE_dataset"
  language: "en"
  image_size: 224

# Model Architecture
model:
  cnn: 
    feature_dim: 512
    dropout: 0.5
  bert:
    model_name:  "michiyasunaga/BioLinkBERT-base"
    freeze_layers: 6
  fusion:
    method: "concatenation"
    hidden_dims: [512, 256]

# Training Configuration
training:
  batch_size: 16
  epochs: 150
  learning_rate: 0.0002
  early_stopping_patience:  25
  multi_task_weights: 
    vqa: 1.0
    segmentation: 0.3
```

---

## ğŸ§ª Technical Details

### Model Architecture

#### 1. Visual Encoder (Custom CNN)
- **Architecture**: 5-layer VGG-inspired network
- **Channels**: 3â†’64â†’128â†’256â†’512â†’512
- **ROI Attention**: Applied at layers 3 and 4
- **Output**:  512-dimensional feature vector
- **Auxiliary Task**: Binary segmentation head

#### 2. Language Encoder (BioBERT)
- **Base Model**: BioLinkBERT-base (110M parameters)
- **Pretraining**: PubMed abstracts + PMC articles
- **Freezing Strategy**: First 6 layers frozen
- **Output**: 768-dimensional question embedding

#### 3. Multimodal Fusion
- **Method**: Concatenation fusion
- **Input**: 512 (visual) + 768 (text) = 1280 dimensions
- **MLP**: 1280â†’512â†’256 with dropout
- **Output**: 256-dimensional fused representation

#### 4. Answer Prediction
- **Task**: Multi-class classification
- **Classes**:  Dynamically determined from dataset
- **Loss**: Weighted Cross-Entropy (handles class imbalance)

#### 5. Knowledge Graph
- **Format**: (head, relation, tail) triplets
- **Retrieval**: Keyword + ROI-based matching
- **Purpose**: Grounding rationales in medical facts

### Training Strategy

- **Multi-task Learning**: Joint VQA + segmentation (0.3 weight)
- **Differential Learning Rates**:
  - CNN: 1e-3 (random initialization)
  - BioBERT: 2e-5 (preserve pretrained knowledge)
  - Fusion: 5e-4 (intermediate)
- **Regularization**:
  - Dropout: 0.3-0.5 across layers
  - Gradient clipping: max norm 1.0
  - Data augmentation: rotation, flip, color jitter
- **Optimization**:
  - Optimizer: Adam with weight decay (5e-5)
  - LR Scheduler: ReduceLROnPlateau (patience:  12)
  - Early stopping: 25 epochs patience

### Explainability Mechanisms

1. **ROI Detection**: Localizes 39 anatomical structures
2. **Attention Maps**:  Visualizes spatial focus (14Ã—14 and 28Ã—28)
3. **Knowledge Retrieval**: Cites relevant medical facts
4. **Rationale Generation**: Template-based explanations
5. **Confidence Scores**: Probability calibration for trust

---

## ğŸ“š Dependencies

Core libraries: 

```
torch>=2.0.0
torchvision>=0.15.0
transformers>=4.30.0
pillow>=9.0.0
numpy>=1.21.0
pandas>=1.3.0
pyyaml>=6.0
tqdm>=4.65.0
matplotlib>=3.5.0
scikit-learn>=1.0.0
tensorboard>=2.10.0
```

See `requirements.txt` for complete list.

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. **Fork the repository**
2. **Create a feature branch**
   ```bash
   git checkout -b feature/YourFeature
   ```
3. **Commit your changes**
   ```bash
   git commit -m "Add YourFeature"
   ```
4. **Push to the branch**
   ```bash
   git push origin feature/YourFeature
   ```
5. **Open a Pull Request**

### Development Guidelines

- Follow PEP 8 style guide
- Add docstrings to all functions
- Write unit tests for new features
- Update documentation as needed

---

## ğŸ› Troubleshooting

### Common Issues

**Issue**:  `CUDA out of memory`
```bash
# Solution:  Reduce batch size in config. yaml
training:
  batch_size: 8  # Instead of 16
```

**Issue**: `FileNotFoundError:  SLAKE dataset not found`
```bash
# Solution: Verify dataset path in config.yaml
dataset:
  root_dir: "./SLAKE_dataset"  # Check this path
```

**Issue**: `ImportError: No module named 'transformers'`
```bash
# Solution:  Reinstall dependencies
pip install -r requirements.txt --upgrade
```

**Issue**: Model predictions are always the same
```bash
# Solution: Ensure model checkpoint is loaded
# Check that best_checkpoint.pth exists in checkpoints/
```

---

## ğŸ“– Citation

If you use VisiHealth AI in your research, please cite:

```bibtex
@software{visihealth2024,
  title={VisiHealth AI:  Explainable Medical Visual Question Answering},
  author={Juni2003},
  year={2024},
  url={https://github.com/juni2003/VisiHealth-AI}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **SLAKE Dataset**: Thanks to the SLAKE team for providing the medical VQA dataset
- **BioLinkBERT**: Built on michiyasunaga's BioLinkBERT model
- **PyTorch**: Powered by the PyTorch deep learning framework
- **Hugging Face**: Transformers library for BERT integration

---

## ğŸ“ Contact

**Developer**:  Juni2003  
**Email**: [your-email@example.com]  
**GitHub**: [@juni2003](https://github.com/juni2003)  
**Project Link**: [https://github.com/juni2003/VisiHealth-AI](https://github.com/juni2003/VisiHealth-AI)

---

## ğŸ—ºï¸ Roadmap

- [ ] **Version 2.0**: Integration with Vision Transformers (ViT)
- [ ] **Web Interface**: Flask/FastAPI deployment
- [ ] **Multi-language**: Support for Chinese questions (SLAKE bilingual)
- [ ] **DICOM Support**: Direct medical imaging format handling
- [ ] **Active Learning**: Human-in-the-loop annotation
- [ ] **Federated Learning**: Privacy-preserving training

---

<div align="center">

**â­ Star this repository if you find it useful!**

Made with â¤ï¸ for advancing medical AI

</div>
## Future Additions (Planned)

The following documents will be added as the project progresses:

- ğŸ“˜ **Mid-Term Report**
- ğŸ“• **Final Year Project Report**
- ğŸ“Š **Evaluation & Results Report**
- ğŸ§ª **Experiment Logs and Analysis**
- ğŸ¤ **Presentation Slides**
- ğŸ“‘ **User Manual / Demo Guide**

All documents will be versioned and updated to reflect incremental progress.

## Note

This repository and its documentation are maintained strictly for **academic and research purposes**.  
No document here represents a clinically deployed system.
